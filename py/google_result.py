#!/usr/bin/env python
#coding=utf-8
#author:youstar
#email:youstar#foxmail.com
import sys,os,urllib,urllib2,string
import re
from htmlentitydefs import name2codepoint
from BeautifulSoup import BeautifulSOAP
 
#get domain
def  getdomain(url):
     patter = 'http(s)?://([\w-]+\.)+[\w-]+'
     m = re.match(patter,url)
     if m is not None:
          return m.group()
#url decode
def html_unescape(str):
     def entity_replacer(m):
          entity = m.group(1)
          if entity in name2codepoint:
               return unichr(name2codepoint[entity])
          else:
               return m.group(0)
     def ascii_replacer(m):
          cp = int(m.group(1))
          if cp <= 255:
               return unichr(cp)
          else:
               return m.group(0)
 
     s =  re.sub(r'&#(\d+);',  ascii_replacer, str, re.U)
     return re.sub(r'&([^;]+);', entity_replacer, s, re.U)
#write to file
#----------------------------------------------------------------------
def writetofile(filename,urltitle):
     hfile = open(filename,'w')
     for key in urltitle.keys():
          lineurl = "%s\r\n"%(urltitle[key])
          hfile.writelines(lineurl)
     hfile.close
#get url link form the result
def geturladdress(keywords,type,number,filename):
     urltitle = {}
     pageid = string.atoi(number)/100
     for idpage in range(0,pageid,1):
          entirehtml= getresponse(keywords,type,idpage*100)
          soup = BeautifulSOAP(entirehtml)
          results = soup.findAll('li', {'class': 'g'})
          for result in results:
               title_a = result.find('a')
               if not title_a:
                    continue
               else:
                    title = ''.join(title_a.findAll(text=True))
                    title = html_unescape(title)
                    #print title
                    url = title_a['href']
                    #print url
                    url = getdomain(url)
                    urltitle[title]= url
     writetofile(filename,urltitle)
#get the response html
def getresponse(keywords,type,number=0):
    if type == "google":
        data = {}
        data['q'] = keywords
        data['start'] = number
        data['num'] = 100
        EnData = urllib.urlencode(data)
        #print EnData
        ggurl = "http://www.google.com.hk/search"
        fullurl = ggurl + "?" + EnData
        #print fullurl
        ggrequest = urllib2.Request(fullurl)
        ggrequest.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)')
        ggresponse = urllib2.urlopen(ggrequest)
        html = ggresponse.read()
        print html
        return html
    elif type == "baidu":
        data = {}
        data['wd']=keywords
        EnData = urllib.urlencode(data)
        baiduurl= "http://www.baidu.com/s?"
        fullurl = baiduurl + EnData
        print fullurl
        bdresponse = urllib2.urlopen(fullurl)
        html = bdresponse.read()
        return html
    else:
        print 'please input the right format'
if __name__=="__main__":
     if len(sys.argv)!= 5:
          print 'please input the right format'
          print 'expamle: spidersearch.py keywords google 3000 export.txt'
     else:
          keywords = sys.argv[1]
          type = sys.argv[2]
          number = sys.argv[3]
          filename = sys.argv[4]
          print 'waiting.....'
          geturladdress(keywords,type,number,filename)
          print 'success.....'
